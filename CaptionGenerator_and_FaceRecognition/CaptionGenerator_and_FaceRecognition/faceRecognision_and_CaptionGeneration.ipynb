{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alok/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-06 16:02:14.547442: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-06 16:02:14.698437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 16:02:15.396640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAddress = \"myModel/snapshots/dc68f91c06a1ba6f15268e5b9c13ae7a7c514084/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(modelAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alok/.local/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(modelAddress)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16\n",
    "num_beams = 4\n",
    "\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(imagePath):\n",
    "    image = Image.open(imagePath)\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    pixelValue = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    outputId = model.generate(pixelValue, **gen_kwargs)\n",
    "\n",
    "    pred = tokenizer.decode(outputId[0], skip_special_tokens=True)\n",
    "    preds = pred.strip()\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a woman walking down the street with a suitcase'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(\"testImages/test1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module for text   \n",
    "from gtts import gTTS # to speech conversion\n",
    "import os # This module is imported so that we can play the converted audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text that you want to convert to audio \n",
    "mytext = predict_step(\"testImages/test1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language in which you want to convert \n",
    "language = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the text and language to the engine,  \n",
    "# here we have marked slow=False. Which tells  \n",
    "# the module that the converted audio should  \n",
    "# have a high speed \n",
    "myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "myobj.save(\"welcome.mp3\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ---> a woman walking down the street with a suitcase\n",
      "2 ---> a red stop sign sitting on the side of a road\n",
      "3 ---> a red traffic light sitting on the side of a road\n",
      "4 ---> a man sitting in a chair with a remote in his hand\n",
      "5 ---> a bed room filled with lots of blankets and pillows\n",
      "6 ---> a broken pipe sitting on the side of a road\n",
      "7 ---> a car driving down a road with a car behind it\n",
      "8 ---> people sitting around a table\n"
     ]
    }
   ],
   "source": [
    "n = 8\n",
    "for i in range(1, n + 1):\n",
    "    print(i, predict_step(f\"testImages/test{i}.jpg\"), sep = \" ---> \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceConfidence(face_distance, face_match_threshold=0.6):\n",
    "    rang = (1 - face_match_threshold)\n",
    "    linear_val = (1 - face_distance) / (rang * 2)\n",
    "\n",
    "    if face_distance > face_match_threshold:\n",
    "        return str(round(linear_val * 100, 2)) + \"%\"\n",
    "    else:\n",
    "        value = (linear_val + ((1 - linear_val) * math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
    "        return str(round(value, 2)) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceConfidence(face_distance, face_match_threshold=0.6):\n",
    "    rang = (1 - face_match_threshold)\n",
    "    linear_val = (1 - face_distance) / (rang * 2)\n",
    "\n",
    "    if face_distance > face_match_threshold:\n",
    "        return str(round(linear_val * 100, 2)) + \"%\"\n",
    "    else:\n",
    "        value = (linear_val + ((1 - linear_val) * math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
    "        return str(round(value, 2)) + \"%\"\n",
    "\n",
    "\n",
    "class faceRecognition:\n",
    "    face_locations = []\n",
    "\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encode_faces()\n",
    "\n",
    "\n",
    "    def encode_faces(self):\n",
    "        for image in os.listdir(\"faces\"):\n",
    "            face_image = face_recognition.load_image_file(\"faces/\" + image)\n",
    "            face_encoding = face_recognition.face_encodings(face_image)[0]\n",
    "\n",
    "            self.known_face_encodings.append(face_encoding)\n",
    "            self.known_face_names.append(image.split(\".\")[0])\n",
    "        \n",
    "    def run_recognition(self, imgPath):\n",
    "        frame = cv2.imread(imgPath)\n",
    "\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1] # BGR to RGB\n",
    "\n",
    "        self.face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        self.face_encodings = face_recognition.face_encodings(rgb_small_frame, self.face_locations)\n",
    "\n",
    "        self.face_names = []\n",
    "        flag = False\n",
    "        name = \"Unknown\"\n",
    "        confidence = \"0%\"\n",
    "        for face_encoding in self.face_encodings:\n",
    "            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "            confidence = \"0%\"\n",
    "\n",
    "            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "\n",
    "            if matches[best_match_index]:\n",
    "                name = self.known_face_names[best_match_index]\n",
    "                confidence = faceConfidence(face_distances[best_match_index])\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if not flag:\n",
    "            return \"Unrecognized Face !\", -1\n",
    "        return name, confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = faceRecognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Unrecognized Face !', -1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.run_recognition(\"testImages/test1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Testing --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(imagePath):\n",
    "    preds = predict_step(imagePath)\n",
    "    person, confidence = fr.run_recognition(imagePath)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if confidence is None:\n",
    "        print(\"Person : Unidentified\")\n",
    "    else:\n",
    "        print(\"Person :\", person)\n",
    "        print(\"Confidence :\", confidence)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Caption :\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Person : Unrecognized Face !\n",
      "Confidence : -1\n",
      "\n",
      "\n",
      "Caption : a laptop computer sitting on top of a table\n"
     ]
    }
   ],
   "source": [
    "predict(\"testImages/test9.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption generation in live cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predictCaption(image):\n",
    "    # image = Image.open(imagePath)\n",
    "    image = Image.fromarray(image)\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    pixelValue = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    outputId = model.generate(pixelValue, **gen_kwargs)\n",
    "\n",
    "    pred = tokenizer.decode(outputId[0], skip_special_tokens=True)\n",
    "    preds = pred.strip()\n",
    "\n",
    "\n",
    "    # -- Face recognision\n",
    "    \n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/alok/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    frameCount = 0\n",
    "\n",
    "    records = set()\n",
    "\n",
    "    while True:\n",
    "        # Read each frame from the webcam\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        x, y, c = frame.shape\n",
    "\n",
    "        # Flip the frame vertically\n",
    "        # frame = cv2.flip(frame, 1)\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if frameCount % 10 == 0:\n",
    "            className = predictCaption(framergb)\n",
    "            records.add(className)\n",
    "\n",
    "\n",
    "        # # show the prediction on the frame\n",
    "        cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, (0,0,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the final output\n",
    "        cv2.imshow(\"Output\", frame) \n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'): # Quits the program by breaking the loop\n",
    "            break\n",
    "\n",
    "        frameCount += 1\n",
    "        frameCount %= 100\n",
    "\n",
    "    # release the webcam and destroy all active windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    recordFile = open(\"records.txt\", \"w\")\n",
    "    for i in records:\n",
    "        recordFile.write(i + \"\\n\")\n",
    "    recordFile.close()\n",
    "\n",
    "except Exception as e:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() \n",
    "    print(\"An error occured !\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# To quit the program, press 'q'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face recognision in liive cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceConfidence(face_distance, face_match_threshold=0.6):\n",
    "    rang = (1 - face_match_threshold)\n",
    "    linear_val = (1 - face_distance) / (rang * 2)\n",
    "\n",
    "    if face_distance > face_match_threshold:\n",
    "        return str(round(linear_val * 100, 2)) + \"%\"\n",
    "    else:\n",
    "        value = (linear_val + ((1 - linear_val) * math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
    "        return str(round(value, 2)) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class faceRecognition:\n",
    "    face_locations = []\n",
    "\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    process_current_frame = True\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encode_faces()\n",
    "\n",
    "\n",
    "    def encode_faces(self):\n",
    "        for image in os.listdir(\"faces\"):\n",
    "            face_image = face_recognition.load_image_file(\"faces/\" + image)\n",
    "            face_encoding = face_recognition.face_encodings(face_image)[0]\n",
    "\n",
    "            self.known_face_encodings.append(face_encoding)\n",
    "            self.known_face_names.append(image.split(\".\")[0])\n",
    "        \n",
    "        print(self.known_face_names)\n",
    "\n",
    "    def run_recognition(self):\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "        if not video_capture.isOpened():\n",
    "            print(\"Cannot open camera\")\n",
    "            exit()\n",
    "\n",
    "        frameCount = 0\n",
    "\n",
    "        className = \"\"\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "\n",
    "            if frameCount % 10 == 0:\n",
    "                framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                \n",
    "                className = predictCaption(framergb)\n",
    "\n",
    "            if self.process_current_frame:\n",
    "                small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "                rgb_small_frame = small_frame[:, :, ::-1] # BGR to RGB\n",
    "\n",
    "                self.face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                self.face_encodings = face_recognition.face_encodings(rgb_small_frame, self.face_locations)\n",
    "\n",
    "                self.face_names = []\n",
    "                for face_encoding in self.face_encodings:\n",
    "                    matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
    "                    name = \"Unknown\"\n",
    "                    confidence = \"0%\"\n",
    "\n",
    "                    face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "\n",
    "                    if matches[best_match_index]:\n",
    "                        name = self.known_face_names[best_match_index]\n",
    "                        confidence = faceConfidence(face_distances[best_match_index])\n",
    "                    \n",
    "                    self.face_names.append(name + \" \" + confidence)\n",
    "            \n",
    "            self.process_current_frame = not self.process_current_frame\n",
    "\n",
    "            for (top, right, bottom, left), name in zip(self.face_locations, self.face_names):\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.5, (0,0,255), 2, cv2.LINE_AA)\n",
    "                \n",
    "            \n",
    "            cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "            frameCount += 1\n",
    "            frameCount %= 100\n",
    "\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "        \n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alok', 'einstien', 'alok', 'einstien', 'alok', 'einstien']\n"
     ]
    }
   ],
   "source": [
    "fr = faceRecognition()\n",
    "fr.run_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
